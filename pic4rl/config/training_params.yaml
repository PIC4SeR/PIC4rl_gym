training_params:
# Trainer parameters
    --max-steps: 1000000 # (int) The maximum steps for training. The default is ``int(1e6)``
    --episode-max-steps: 600 # (int) The maximum steps for an episode. The default is ``int(1e3)``
    --n-experiments: 1 # (int) Number of experiments. The default is ``1``
# --show-progress: # Call ``render`` function during training
    --save-model-interval: 10000 # (int) Interval to save model. The default is ``int(1e4)``
    --save-summary-interval: 10000 # (int) Interval to save summary. The default is ``int(1e3)``
#    --model-dir: 'src/PIC4rl_gym/pic4rl/trained_agents/vineyards/20231017T211306.665432_SAC_' # (str) Directory to restore model.
#    --dir-suffix: ""  # (str) Suffix for directory that stores results.
#    --normalize-obs: # Whether normalize observation
    --logdir: 'src/Results' # (str) Output directory name. The default is ``"results"`` (it is relative to workspace directory).

# ENABLE TESTING MODE    
#    --evaluate: THIS FLAG ENABLE TESTING MODE
    --test-interval: 10000 # (int) Interval to evaluate trained model. The default is ``int(1e4)``
# --show-test-progress: # Call ``render`` function during evaluation.
    --test-episodes: 1 # (int) Number of episodes at validation to be averaged. # IF TESTING MODE SET TO 1, default validation '3' or '5'
# --save-test-path: # Save trajectories of evaluation.
# --show-test-images: # Show input images to neural networks when an episode finishes
# --save-test-movie: # Save rendering results.
    --use-prioritized-rb: # Use prioritized experience replay
    --use-nstep-rb: # Use Nstep experience replay
    --n-step: 4 # (int) Number of steps for nstep experience reward. The default is ``4``
    #--restore-rb: # Load experience replay buffer
    #--rb-path: 'src/Results/vineyard/last_rb_SAC_150k_pr_ns4.npz'
    --logging-level: INFO # (DEBUG, INFO, WARNING) Choose logging level. The default is ``INFO``
    --policy: 'SAC' # training policy
    --policy_trainer: 'off-policy' # trainer off-policy / on-policy
    --change_goal_and_pose: 3
    --starting_episodes: 0
    --batch-size: 64 # (int) Batch size for training. The default is ``32``.
    --gpu: 0 # (int) GPU id. ``-1`` disables GPU. The default is ``0``.

# For all off-policy algos (DDPG, TD3, SAC)
    --n-warmup: 15000 # (int) Number of warmup steps before training. The default is ``1e4``.
    --memory-capacity: 200000  # (int) Replay Buffer size. The default is ``1e6``.

# For only SAC
    --alpha: 0.2 # (float) Temperature parameter. The default is ``0.2``.
    --auto-alpha: # Automatic alpha tuning
# For only SAC-AE
    # --stop-q-grad: # Whether stop gradient after convolutional layers at Encoder

# For on-policy algorithms (PPO)
    # --horizon: 2048  # (int) The default is ``2048``.
    # --normalize-adv: # Normalize Advantage. default True
    # --enable-gae: # Enable GAE. Default True

# Testing with tflite models
    --tflite_flag: False
    --tflite_model_path: '~/inference/actor_fp16.tflite'
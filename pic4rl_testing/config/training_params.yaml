training_params:
# Trainer parameters
    --max-steps: 1200000 # (int) The maximum steps for training. The default is ``int(1e6)``
    --episode-max-steps: 1000 # (int) The maximum steps for an episode. The default is ``int(1e3)``
    --n-experiments: 3 # (int) Number of experiments. The default is ``1``
# --show-progress: # Call ``render`` function during training
    --save-summary-interval: 5000 # (int) Interval to save summary. The default is ``int(1e3)``
    --model-dir: '/root/gym_ws/src/Results/20231013_201633.102241_camera_SAC/' # (str) Directory to restore model.
#    --dir-suffix: # (str) Suffix for directory that stores results.
#    --normalize-obs: # Whether normalize observation
    --logdir: '/root/gym_ws/src/Results/testing/' # (str) Output directory name. The default is ``"results"``
    --evaluate: # Whether evaluate trained model
    --test-interval: 20000 # (int) Interval to evaluate trained model. The default is ``int(1e4)``
# --show-test-progress: # Call ``render`` function during evaluation.
    --test-episodes: 3 # (int) Number of episodes at test. The default is ``5``
# --save-test-path: # Save trajectories of evaluation.
# --show-test-images: # Show input images to neural networks when an episode finishes
# --save-test-movie: # Save rendering results.
    --logging-level: INFO # (DEBUG, INFO, WARNING) Choose logging level. The default is ``INFO``
    --policy: 'SAC' # training policy
    --policy_trainer: 'off-policy' # trainer off-policy / on-policy
    --change_goal_and_pose: 1
    --starting_episodes: 0
    --tflite_flag: False
    --tflite_model_path: '~/inference/actor_fp16.tflite'
    --batch-size: 32 # (int) Batch size for training. The default is ``32``.
    --gpu: 0 # (int) GPU id. ``-1`` disables GPU. The default is ``0``.

# For all off-policy algos (DDPG, TD3, SAC)
    --memory-capacity: 10000  # (int) Replay Buffer size. The default is ``1e6``.

# For only SAC
    # --alpha: 0.2 # (float) Temperature parameter. The default is ``0.2``.
    # --auto-alpha: # Automatic alpha tuning
# For only SAC-AE
    # --stop-q-grad: # Whether stop gradient after convolutional layers at Encoder

# For on-policy algorithms (PPO)
    # --horizon: 2048  # (int) The default is ``2048``.
    # --normalize-adv: # Normalize Advantage. default True
    # --enable-gae: # Enable GAE. Default True
